mutate(period = if_else(Off.Peak == 0, "Peak", "Off-Peak")) %>%
ggplot(aes(x = factor(Route), fill = period)) +
geom_bar(position = "dodge") +
labs(title = "Ridership by Route (Peak vs Off-Peak)",
x = "Route", y = "Rider Counts") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
# heatmap; route vs time of day
ridership_data %>%
add_time_of_day() %>%
count(Route, time_of_day) %>%
ggplot(aes(x = factor(Route), y = time_of_day, fill = n)) +
geom_tile() +
scale_fill_viridis_c() +
labs(title = "Ridership Heatmap: Route Ã— Time of Day",
x = "Route", y = "Time of Day", fill = "Riders") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
# distribution of peak start times
peak %>%
ggplot(aes(x = hour, fill = time_of_day)) +
geom_histogram(binwidth = 1) +
labs(title = "Distribution of Trip Times (Peak Period)",
x = "Hour of Day", y = "Count") +
theme_minimal()
# Step 3: Summarizing route performance (function to apply to peak and non-peak)
#' @description this function takes in data with bus routes and the difference
#' between their scheduled arrival times and actual arrival times, categorizes
#' them as on time, early, or late and then summarizes the percentage of the
#' time each route is on time, early, or late
#' @param a dataframe with data documenting each trip in a bus system over a
#' set period of time, with variables for the route number and difference
#' between scheduled and actual arrival time
#' @return a dataframe with a column for each route number and the share of
#' trips that were on time, late, or early
route_performance <- function(data = otp_data_updated){
data <- data %>%
mutate(OnTime = case_when(Delay.Sec <= -60 ~ "Early",
Delay.Sec >= 300 ~ "Late",
Delay.Sec > -60 & Delay.Sec < 300 ~  "On Time")
)
route_performance <- data %>%
group_by(Route) %>%
summarize(
Share_early = mean(OnTime == "Early"),
Share_on_time = mean(OnTime == "On Time"),
Share_late = mean(OnTime == "Late")
)
}
route_performance_results <- route_performance(otp_data_updated)
# Step 5: Merging data frames (for peak and non-peak)
merged_data_peak <- right_join(route_performance_results, peak_summary,
by = "Route")
# Step 4: Summarizing demographic exposure (function to apply to peak + non-peak)
library(tidyverse)
# REMINDER!! do the roxygen function descriptions
summarize_ridership_demographics <- function(ridership_df) {
# classify demographic groups
classify_groups <- function(df) {
df %>%
mutate(
HS = case_when(
!is.na(High.School) & High.School != "None" ~ 1,
TRUE ~ 0
),
Brown_RISD = case_when(
College != "None" ~ 1,
TRUE ~ 0
),
Senior = case_when(
Type == "Senior" ~ 1,
TRUE ~ 0
),
Disabled = case_when(
Type == "Disabled" ~ 1,
TRUE ~ 0
),
Low_Income = Low.Income  # already binary
)
}
# summarize by route
summarize_routes <- function(df) {
df %>%
group_by(Route) %>%
summarise(
HS = sum(HS, na.rm = TRUE),
College = sum(Brown_RISD, na.rm = TRUE),
Senior = sum(Senior, na.rm = TRUE),
Disabled = sum(Disabled, na.rm = TRUE),
Low_Income = sum(Low_Income, na.rm = TRUE),
.groups = "drop"
)
}
# split into peak / off-peak
peak_df <- ridership_df %>% filter(Off.Peak == 0)
offpeak_df <- ridership_df %>% filter(Off.Peak == 1)
# classify demographics
full_df <- classify_groups(ridership_df)
peak_df <- classify_groups(peak_df)
offpeak_df <- classify_groups(offpeak_df)
# summarize by route, raw numbers for visualization
all_summary <- summarize_routes(full_df)
peak_summary <- summarize_routes(peak_df)
offpeak_summary <- summarize_routes(offpeak_df)
# summarize by route (proportion tables)
all_prop_summary <- summarize_routes(full_df) %>%
rowwise() %>%
mutate(
total = sum(c_across(-Route)),
across(-c(Route, total), ~ .x / total)
) %>%
ungroup() %>%
select(-total)
peak_prop_summary <- summarize_routes(peak_df) %>%
rowwise() %>%
mutate(
total = sum(c_across(-Route)),
across(-c(Route, total), ~ .x / total)
) %>%
ungroup() %>%
select(-total)
offpeak_prop_summary <- summarize_routes(offpeak_df) %>%
rowwise() %>%
mutate(
total = sum(c_across(-Route)),
across(-c(Route, total), ~ .x / total)
) %>%
ungroup() %>%
select(-total)
# make visualizations helper
make_plot <- function(summary_df, title_text) {
summary_df %>%
pivot_longer(cols = -Route, names_to = "Group", values_to = "Count") %>%
ggplot(aes(x = factor(Route), y = Count, fill = Group)) +
geom_col(position = "stack") +
scale_y_continuous(labels = scales::comma) +
labs(
title = title_text,
x = "Route",
y = "Ridership Count",
fill = "Demographic Group"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
}
peak_plot <- make_plot(peak_summary, "Peak Ridership Demographic Breakdown by Route")
offpeak_plot <- make_plot(offpeak_summary, "Off-Peak Ridership Demographic Breakdown by Route")
# return tidy output
tibble(
overall_summary = list(all_prop_summary),
peak_summary = list(peak_prop_summary),
offpeak_summary = list(offpeak_prop_summary),
plots = list(
tibble(
peak_plot = list(peak_plot),
offpeak_plot = list(offpeak_plot)
)
)
)
}
result <- summarize_ridership_demographics(ridership_data)
overall_summary <- result$overall_summary[[1]]
peak_summary <- result$peak_summary[[1]]
off_summary <- result$offpeak_summary[[1]]
result$plots[[1]]$peak_plot[[1]]
result$plots[[1]]$offpeak_plot[[1]]
# peak vs off peak comparison of single demo. (Seniors)
# NOTE: change to only include the top most-used routes
bind_rows(
peak_summary  %>% mutate(period = "Peak"),
off_summary %>% mutate(period = "Off-Peak")
) %>%
ggplot(aes(x = factor(Route), y = Senior, fill = period)) +
geom_col(position = "dodge") +
labs(
title = "Senior Ridership: Peak vs Off-Peak",
x = "Route", y = "Count or Proportion", fill = "Period"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
# Step 5: Merging data frames (for peak and non-peak)
merged_data_peak <- right_join(route_performance_results, peak_summary,
by = "Route")
merged_data_off_peak <- right_join(route_performance_results, off_summary,
by = "Route")
merged_data <- right_join(route_performance_results, overall_summary,
by = "Route")
# Step 6: Correlation and hypothesis tests
correlations <- function(df, threshold = 0.6, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
cor = vector("numeric"))
# get rid of any NaN
df <- select_if(df, is.numeric)
# find correlations
cor_mat <- cor(df, use = use)
# find pairs with high correlations
n <- nrow(cor_mat)
for( i in 1:(n-1)){
for(j in (i + 1):n){
if(!is.na(cor_mat[i, j]) && abs(cor_mat[i, j]) > threshold){
res <- add_row(res,
name1 = colnames(cor_mat)[i],
name2 = colnames(cor_mat)[j],
cor = cor_mat[i,j])
}
}
}
return(res)
}
correlation_data_peak <- correlations(df = merged_data_peak, threshold = 0.4,
use = "everything")
correlation_data_off_peak <- correlations(df = merged_data_off_peak,
threshold = 0.4, use = "everything")
correlation_data_overall <- correlations(df = merged_data,
threshold = 0.4, use = "everything")
t_tests <- function(df, threshold = 0.05, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
t_test = vector("numeric"))
# find t tests and pairs with statistically significant t tests
for( i in 1:(n-1)){
for(j in (i + 1):n){
test_result <- t.test(df[[i]], df[[j]], use = use)
if(test_result$p.value < threshold){
res <- add_row(res,
name1 = colnames(df)[i],
name2 = colnames(df)[j],
t_test = test_result$p.value)
}
}
}
return(res)
}
t_tests_data <- t_tests(merged_data_peak)
View(otp_data)
View(peak)
View(otp_data)
?str_split_1
otp_data_updated <- str_split_1(otp_data_updated$Scheduled.Time, " ")
otp_data_updated <- str_split_1(otp_data_updated$Scheduled.Time, " ")
otp_data_updated <- str_split(otp_data_updated$Scheduled.Time, " ")
View(otp_data_updated)
otp_data_updated <- subset(otp_data, select = - c(Driver.ID, Trip, Stop, Stop.Sequence,
StopLat, StopLng))
str_split_fixed(otp_data_updated$Scheduled.Time, " ", 2)
otp_data_updated <- str_split_fixed(otp_data_updated$Scheduled.Time, " ", 2)
View(otp_data_updated)
# arranging in order of routes
otp_data_updated <- otp_data %>%
arrange(Route)
otp_data_updated <- subset(otp_data, select = - c(Driver.ID, Trip, Stop, Stop.Sequence,
StopLat, StopLng))
View(otp_data_updated)
otp_data_updated <- otp_data_updated %>%
str_split_fixed(Scheduled.Time, " ", 2)
split_time <- str_split_fixed(otp_data_updated$Scheduled.Time, " ", 2)
otp_data_updated$Scheduled_Date <- split_time[,1]
otp_data_updated$Scheduled_Time <- split_time[,2]
View(otp_data_updated)
otp_data_updated$Scheduled_Date <- weekdays(as.Date(otp_data_updated$Scheduled_Date))
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday")
View(otp_data_peak)
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday") %>%
filter(between(Scheduled.Time, 07:00:00, 09:00:00) |
between(Scheduled.Time, 15:00:00, 18:00:00))
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday") %>%
mutate(Scheduled.Time = as_hms(Scheduled.Time))
library(hms)
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday") %>%
mutate(Scheduled.Time = as_hms(Scheduled.Time))
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday") %>%
as_hms(Scheduled.Time)
?as_hms
as_hms(otp_data_updated$Scheduled.Time)
otp_data_updated <- otp_data_updated %>%
mutate(Scheduled.Time = as_hms(Scheduled.Time))
otp_data_updated <- otp_data_updated %>%
filter(Scheduled.Time != "" & !is.na(Scheduled.Time))
mutate(Scheduled.Time = as_hms(Scheduled.Time))
otp_data_updated <- otp_data_updated %>%
filter(Scheduled_Time != "" & !is.na(Scheduled_Time))
mutate(Scheduled_Time = as_hms(Scheduled_Time))
otp_data_updated <- otp_data_updated %>%
filter(Scheduled_Time != "" & !is.na(Scheduled_Time)) %>%
mutate(Scheduled_Time = as_hms(Scheduled_Time))
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday") %>%
filter(between(Scheduled_Time, 07:00:00, 09:00:00) |
between(Scheduled_Time, 15:00:00, 18:00:00))
otp_data_peak <- otp_data_updated %>%
filter(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday") %>%
filter(between(Scheduled_Time, as_hms("07:00:00"), as_hms("09:00:00")) |
between(Scheduled_Time, as_hms("15:00:00"), as_hms("18:00:00")))
View(otp_data_peak)
otp_data_off_peak <- otp_data_updated %>%
filter(Scheduled_Date == "Saturday" | Scheduled_Date == "Sunday" |
(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday" &
(Scheduled_Time < as_hms("07:00:00") |
between(Scheduled_Time, as_hms("09:00:00"), as_hms("15:00:00")) |
Scheduled_Time > as_hms("18:00:00")))))
otp_data_off_peak <- otp_data_updated %>%
filter(Scheduled_Date == "Saturday" | Scheduled_Date == "Sunday" |
(Scheduled_Date != "Saturday" & Scheduled_Date != "Sunday" &
(Scheduled_Time < as_hms("07:00:00") |
between(Scheduled_Time, as_hms("09:00:00"), as_hms("15:00:00")) |
Scheduled_Time > as_hms("18:00:00"))))
View(otp_data_off_peak)
route_performance_peak <- route_performance(otp_data_peak)
route_performance_overall <- route_performance(otp_data_updated)
route_performance_off_peak <- route_performance(otp_data_off_peak)
merged_data_peak <- right_join(route_performance_peak, peak_summary,
by = "Route")
merged_data_off_peak <- right_join(route_performance_off_peak, off_summary,
by = "Route")
merged_data <- right_join(route_performance_overall, overall_summary,
by = "Route")
correlations <- function(df, threshold = 0.6, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
cor = vector("numeric"))
# get rid of any NaN
df <- select_if(df, is.numeric)
# find correlations
cor_mat <- cor(df, use = use)
# find pairs with high correlations
n <- nrow(cor_mat)
for( i in 1:(n-1)){
for(j in (i + 1):n){
if(!is.na(cor_mat[i, j]) && abs(cor_mat[i, j]) > threshold){
res <- add_row(res,
name1 = colnames(cor_mat)[i],
name2 = colnames(cor_mat)[j],
cor = cor_mat[i,j])
}
}
}
return(res)
}
correlation_data_peak <- correlations(df = merged_data_peak, threshold = 0.4,
use = "everything")
correlation_data_off_peak <- correlations(df = merged_data_off_peak,
threshold = 0.4, use = "everything")
correlation_data_overall <- correlations(df = merged_data,
threshold = 0.4, use = "everything")
View(correlation_data_off_peak)
View(correlation_data_overall)
View(correlation_data_peak)
t_tests <- function(df, threshold = 0.05, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
t_test = vector("numeric"))
n <- ncol(df)
# find t tests and pairs with statistically significant t tests
for( i in 1:(n-1)){
for(j in (i + 1):n){
test_result <- t.test(df[[i]], df[[j]])
if(test_result$p.value < threshold){
res <- rbind(res, data.frame(
name1 = colnames(df)[i],
name2 = colnames(df)[j],
t_test = test_result$p.value))
}
}
}
return(res)
}
t_tests_data <- t_tests(merged_data_peak)
View(t_tests_data)
View(t_tests_data)
t_tests_peak <- t_tests(merged_data_peak)
t_tests_off_peak <- t_tests(merged_data_off_peak)
t_tests_overall <- t_tests(merged_data)
View(t_tests_off_peak)
View(merged_data)
correlations <- function(df, threshold = 0.6, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
cor = vector("numeric"))
# get rid of any NaN
df <- select_if(df, is.numeric)
# find correlations
cor_mat <- cor(df, use = use)
# find pairs with high correlations
n <- nrow(cor_mat)
for( i in 1:(n-1)){
for(j in (i + 1):n){
if(!is.na(cor_mat[i, j]) && abs(cor_mat[i, j]) > threshold){
res <- add_row(res,
name1 = colnames(cor_mat)[i],
name2 = colnames(cor_mat)[j],
cor = cor_mat[i,j])
}
}
}
res <- res %>%
filter(name1 == "Share_on_time" | name2 == "Share_on_time" |
name1 == "Share_early" | name2 == "Share_early" |
name1 == "Share_late" | name2 == "Share_late")
return(res)
}
correlation_data_peak <- correlations(df = merged_data_peak, threshold = 0.4,
use = "everything")
correlation_data_off_peak <- correlations(df = merged_data_off_peak,
threshold = 0.4, use = "everything")
correlation_data_overall <- correlations(df = merged_data,
threshold = 0.4, use = "everything")
View(correlation_data_peak)
View(correlation_data_overall)
View(correlation_data_off_peak)
t_tests <- function(df, threshold = 0.05, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
t_test = vector("numeric"))
n <- ncol(df)
# find t tests and pairs with statistically significant t tests
for( i in 1:(n-1)){
for(j in (i + 1):n){
test_result <- t.test(df[[i]], df[[j]])
if(test_result$p.value < threshold){
res <- rbind(res, data.frame(
name1 = colnames(df)[i],
name2 = colnames(df)[j],
t_test = test_result$p.value))
}
}
}
res <- res %>%
filter(name1 == "Share_on_time" | name2 == "Share_on_time" |
name1 == "Share_early" | name2 == "Share_early" |
name1 == "Share_late" | name2 == "Share_late")
return(res)
}
t_tests_peak <- t_tests(merged_data_peak)
t_tests_off_peak <- t_tests(merged_data_off_peak)
t_tests_overall <- t_tests(merged_data)
View(t_tests_off_peak)
# Step 6: Correlation and hypothesis tests
correlations <- function(df, threshold = 0.6, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
cor = vector("numeric"))
# get rid of any NaN
df <- select_if(df, is.numeric)
# find correlations
cor_mat <- cor(df, use = use)
# find pairs with high correlations
n <- nrow(cor_mat)
for( i in 1:(n-1)){
for(j in (i + 1):n){
if(!is.na(cor_mat[i, j]) && abs(cor_mat[i, j]) > threshold){
res <- add_row(res,
name1 = colnames(cor_mat)[i],
name2 = colnames(cor_mat)[j],
cor = cor_mat[i,j])
}
}
}
res <- res %>%
filter(name1 == "Share_on_time" | name2 == "Share_on_time" |
name1 == "Share_early" | name2 == "Share_early" |
name1 == "Share_late" | name2 == "Share_late") %>%
filter(name1 != "Route" & name2 != "Route")
return(res)
}
correlation_data_peak <- correlations(df = merged_data_peak, threshold = 0.4,
use = "everything")
correlation_data_off_peak <- correlations(df = merged_data_off_peak,
threshold = 0.4, use = "everything")
correlation_data_overall <- correlations(df = merged_data,
threshold = 0.4, use = "everything")
t_tests <- function(df, threshold = 0.05, use = "everything"){
# results data frame
res <- data.frame(name1 = vector("character"),
name2 = vector("character"),
t_test = vector("numeric"))
n <- ncol(df)
# find t tests and pairs with statistically significant t tests
for( i in 1:(n-1)){
for(j in (i + 1):n){
test_result <- t.test(df[[i]], df[[j]])
if(test_result$p.value < threshold){
res <- rbind(res, data.frame(
name1 = colnames(df)[i],
name2 = colnames(df)[j],
t_test = test_result$p.value))
}
}
}
res <- res %>%
filter(name1 == "Share_on_time" | name2 == "Share_on_time" |
name1 == "Share_early" | name2 == "Share_early" |
name1 == "Share_late" | name2 == "Share_late") %>%
filter(name1 != "Route" & name2 != "Route")
return(res)
}
t_tests_peak <- t_tests(merged_data_peak)
t_tests_off_peak <- t_tests(merged_data_off_peak)
t_tests_overall <- t_tests(merged_data)
View(t_tests_peak)
